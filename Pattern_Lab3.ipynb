{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVoZIhQuOq2Q",
        "outputId": "60df11fb-b68b-4e2f-a824-73ee2f7a3a9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/image-classification\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"duttadebadri/image-classification\")\n",
        "path1 = kagglehub.dataset_download(\"steubk/wikiart\") #this is 31.4GB, BIG PROBLEM\n",
        "#print(\"Path to dataset files:\", path, path1)\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ikfBXLr7QF_k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WHEN WE ADD PATH1"
      ],
      "metadata": {
        "id": "TOu_659sUGzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(content_path, style_path, max_total_images=10000, content_ratio=0.8, output_dir=\"dataset_splits\"):\n",
        "    content_path = Path(content_path)\n",
        "    style_path = Path(style_path)\n",
        "\n",
        "    # Collect image files\n",
        "    content_images = list(content_path.rglob(\"*.jpg\")) + list(content_path.rglob(\"*.png\"))\n",
        "    style_images = list(style_path.rglob(\"*.jpg\")) + list(style_path.rglob(\"*.png\"))\n",
        "\n",
        "    # Shuffle\n",
        "    random.shuffle(content_images)\n",
        "    random.shuffle(style_images)\n",
        "\n",
        "    # Sample according to the max_total_images and ratio\n",
        "    content_count = int(max_total_images * content_ratio)\n",
        "    style_count = max_total_images - content_count\n",
        "    content_images = content_images[:content_count]\n",
        "    style_images = style_images[:style_count]\n",
        "\n",
        "    # Combine and shuffle again\n",
        "    all_images = content_images + style_images\n",
        "    random.shuffle(all_images)\n",
        "\n",
        "    # Split: 70% train, 20% val, 10% test\n",
        "    train_split = int(len(all_images) * 0.7)\n",
        "    val_split = int(len(all_images) * 0.2)\n",
        "\n",
        "    train_images = all_images[:train_split]\n",
        "    val_images = all_images[train_split:train_split + val_split]\n",
        "    test_images = all_images[train_split + val_split:]\n",
        "    return train_images, val_images, test_images\n",
        "\n"
      ],
      "metadata": {
        "id": "0jRQSuJKPj6s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_files(images, split_name, output_dir):\n",
        "    split_dir = Path(output_dir) / split_name\n",
        "    split_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for img_path in images:\n",
        "        dest = split_dir / img_path.name\n",
        "        shutil.copy(img_path, dest)\n",
        "\n",
        "train_images, val_images, test_images = split_dataset(path, path1)\n",
        "\n",
        "copy_files(train_images, \"train\", \"dataset_splits\")\n",
        "copy_files(val_images, \"val\", \"dataset_splits\")\n",
        "copy_files(test_images, \"test\", \"dataset_splits\")\n",
        "\n",
        "print(f\"Dataset split completed: {len(train_images)} train, {len(val_images)} val, {len(test_images)} test.\")\n",
        "\n",
        "\n",
        "print(f\"Dataset split completed: {len(train_images)} train, {len(val_images)} val, {len(test_images)} test.\")\n",
        "\n",
        "# Use the function on the downloaded paths\n",
        "split_dataset(path, path1)"
      ],
      "metadata": {
        "id": "6T-h5ekLRuhH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(img_path, image_size=(256, 256)):\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img = img.resize(image_size)\n",
        "    img_arr = np.array(img) / 255.0\n",
        "    return img_arr.astype(np.float32)"
      ],
      "metadata": {
        "id": "FasTnOHERY0z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_split_dataset(split_dir=\"dataset_splits\", image_size=(256, 256), output_dir=\"processed_dataset\"):\n",
        "    split_dir = Path(split_dir)\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        split_path = split_dir / split\n",
        "        images = list(split_path.glob(\"*.jpg\")) + list(split_path.glob(\"*.png\"))\n",
        "        random.shuffle(images)\n",
        "\n",
        "        half = len(images) // 2\n",
        "        content_imgs = images[:half]\n",
        "        style_imgs = images[half:]\n",
        "\n",
        "        triplets = []\n",
        "        for content_img in content_imgs:\n",
        "            style_img = random.choice(style_imgs)\n",
        "            try:\n",
        "                c_img = preprocess_image(content_img, image_size)\n",
        "                s_img = preprocess_image(style_img, image_size)\n",
        "                threshold = np.random.uniform(0.2, 1.0)\n",
        "                triplets.append((c_img, s_img, threshold))\n",
        "            except Exception as e:\n",
        "                print(f\"[{split}] Skipped image due to error: {e}\")\n",
        "\n",
        "        split_out = output_dir / f\"{split}_triplets.npz\"\n",
        "        np.savez_compressed(split_out, data=triplets)\n",
        "        print(f\"[{split}] Saved {len(triplets)} triplets to {split_out}\")\n"
      ],
      "metadata": {
        "id": "KmTEHN0CRpZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then preprocess the split images into triplets\n",
        "preprocess_split_dataset()\n"
      ],
      "metadata": {
        "id": "1NYPG1vgR0qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding Model"
      ],
      "metadata": {
        "id": "nWv_xg27VS2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class VGGFeatureExtractor(nn.Module):\n",
        "    def __init__(self, layers=None):\n",
        "        super(VGGFeatureExtractor, self).__init__()\n",
        "        if layers is None:\n",
        "            # Layers to extract: relu1_1, relu2_1, relu3_1, relu4_1, relu5_1\n",
        "            self.layers = ['0', '5', '10', '19', '28']\n",
        "\n",
        "        vgg = models.vgg19(pretrained=True).features.eval()\n",
        "        for param in vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.vgg = vgg\n",
        "        self.selected_layers = self.layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for name, layer in self.vgg._modules.items():\n",
        "            x = layer(x)\n",
        "            if name in self.selected_layers:\n",
        "                features.append(x)\n",
        "        return features\n"
      ],
      "metadata": {
        "id": "EbE0PfWLVBQf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gram_matrix(tensor):\n",
        "    b, c, h, w = tensor.size()\n",
        "    tensor = tensor.view(b, c, h * w)\n",
        "    gram = torch.bmm(tensor, tensor.transpose(1, 2))\n",
        "    return gram / (c * h * w)\n"
      ],
      "metadata": {
        "id": "WPdYRTP5VV_H"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleTransferModel(nn.Module):\n",
        "    def __init__(self, content_weight=1e5, style_weight=1e10):\n",
        "        super(StyleTransferModel, self).__init__()\n",
        "        self.vgg = VGGFeatureExtractor()\n",
        "        self.content_weight = content_weight\n",
        "        self.style_weight = style_weight\n",
        "\n",
        "    def forward(self, content_img, style_img, generated_img):\n",
        "        content_features = self.vgg(content_img)\n",
        "        style_features = self.vgg(style_img)\n",
        "        gen_features = self.vgg(generated_img)\n",
        "\n",
        "        # Compute content loss (e.g., relu4_1)\n",
        "        content_loss = torch.mean((gen_features[3] - content_features[3]) ** 2)\n",
        "\n",
        "        # Compute style loss (all selected layers)\n",
        "        style_loss = 0\n",
        "        for gf, sf in zip(gen_features, style_features):\n",
        "            gram_g = gram_matrix(gf)\n",
        "            gram_s = gram_matrix(sf)\n",
        "            style_loss += torch.mean((gram_g - gram_s) ** 2)\n",
        "\n",
        "        total_loss = self.content_weight * content_loss + self.style_weight * style_loss\n",
        "        return total_loss\n"
      ],
      "metadata": {
        "id": "87kYrK2UVXsz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def closure(model, optimizer, content_img, style_img, generated_img):\n",
        "    optimizer.zero_grad()\n",
        "    loss = model(content_img, style_img, generated_img)\n",
        "    loss.backward()\n",
        "    return loss\n",
        "\n",
        "def stylize_image(content_img, style_img, model, num_steps=300, style_threshold=1.0):\n",
        "    generated = content_img.clone().requires_grad_(True)\n",
        "    optimizer = torch.optim.LBFGS([generated])\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        optimizer.step(lambda: closure(model, optimizer, content_img, style_img, generated))\n",
        "\n",
        "    # Apply style threshold blending\n",
        "    with torch.no_grad():\n",
        "        final_img = style_threshold * generated + (1 - style_threshold) * content_img\n",
        "\n",
        "    return final_img.detach()\n"
      ],
      "metadata": {
        "id": "v7pN9s7YccRi"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Want to avoid functions inside functions\n",
        "# def stylize_image(content_img, style_img, model, num_steps=300, style_threshold=1.0):\n",
        "#     generated = content_img.clone().requires_grad_(True)\n",
        "#     optimizer = torch.optim.LBFGS([generated])\n",
        "\n",
        "#     for step in range(num_steps):\n",
        "#         def closure():\n",
        "#             optimizer.zero_grad()\n",
        "#             loss = model(content_img, style_img, generated)\n",
        "#             loss.backward()\n",
        "#             return loss\n",
        "#         optimizer.step(closure)\n",
        "\n",
        "#     return generated.detach()\n"
      ],
      "metadata": {
        "id": "k-U8pW7KVajZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset loader\n",
        "class StyleTransferDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, npz_path):\n",
        "        data = np.load(npz_path, allow_pickle=True)['data']\n",
        "        self.triplets = data.tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triplets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        c, s, threshold = self.triplets[idx]\n",
        "        return (\n",
        "            torch.tensor(c).permute(2, 0, 1),   # (3,H,W)\n",
        "            torch.tensor(s).permute(2, 0, 1),\n",
        "            torch.tensor(threshold, dtype=torch.float32)\n",
        "        )\n"
      ],
      "metadata": {
        "id": "iPI8i7nNdPCT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, upsample=False):\n",
        "        super().__init__()\n",
        "        if upsample:\n",
        "            self.block = nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "                nn.InstanceNorm2d(out_channels, affine=True),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "        else:\n",
        "            self.block = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "                nn.InstanceNorm2d(out_channels, affine=True),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n"
      ],
      "metadata": {
        "id": "2_wTz2usfuuw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.InstanceNorm2d(channels, affine=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.InstanceNorm2d(channels, affine=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n"
      ],
      "metadata": {
        "id": "pPMNfCQXfybh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TransformerNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Initial convolution layers\n",
        "        self.down = nn.Sequential(\n",
        "            ConvBlock(3, 32, 9, 1, 4),    # 256 -> 256\n",
        "            ConvBlock(32, 64, 3, 2, 1),   # 256 -> 128\n",
        "            ConvBlock(64, 128, 3, 2, 1),  # 128 -> 64\n",
        "        )\n",
        "        # 5 Residual blocks\n",
        "        self.residuals = nn.Sequential(\n",
        "            *[ResidualBlock(128) for _ in range(5)]\n",
        "        )\n",
        "        # Upsampling\n",
        "        self.up = nn.Sequential(\n",
        "            ConvBlock(128, 64, 3, 1, 1, upsample=True),  # 64 -> 128\n",
        "            ConvBlock(64, 32, 3, 1, 1, upsample=True),   # 128 -> 256\n",
        "            nn.Conv2d(32, 3, 9, 1, 4),                   # final conv\n",
        "            nn.Tanh()  # optional, for keeping outputs in range [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x, style_threshold):\n",
        "        y = self.down(x)\n",
        "        y = self.residuals(y)\n",
        "        y = self.up(y)\n",
        "        return style_threshold * y + (1 - style_threshold) * x\n"
      ],
      "metadata": {
        "id": "08mhh9SxdVn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NSTLoss(nn.Module):\n",
        "    def __init__(self, content_weight=1e5, style_weight=1e10):\n",
        "        super().__init__()\n",
        "        self.vgg = VGGFeatureExtractor()\n",
        "        self.content_weight = content_weight\n",
        "        self.style_weight = style_weight\n",
        "\n",
        "    def forward(self, content_img, style_img, stylized_img):\n",
        "        c_feats = self.vgg(content_img)\n",
        "        s_feats = self.vgg(style_img)\n",
        "        g_feats = self.vgg(stylized_img)\n",
        "\n",
        "        content_loss = torch.mean((g_feats[3] - c_feats[3]) ** 2)\n",
        "        style_loss = 0\n",
        "        for gf, sf in zip(g_feats, s_feats):\n",
        "            gram_g = gram_matrix(gf)\n",
        "            gram_s = gram_matrix(sf)\n",
        "            style_loss += torch.mean((gram_g - gram_s) ** 2)\n",
        "\n",
        "        return self.content_weight * content_loss + self.style_weight * style_loss\n"
      ],
      "metadata": {
        "id": "TRHqLbDsdkEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, loss_fn, device, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for i, (content, style, threshold) in enumerate(dataloader):\n",
        "            content = content.to(device)\n",
        "            style = style.to(device)\n",
        "            threshold = threshold.to(device).view(-1, 1, 1, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            generated = model(content, threshold)\n",
        "            loss = loss_fn(content, style, generated)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(dataloader)\n",
        "        print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Evaluate on validation set every epoch\n",
        "        val_loss = evaluate(model, val_loader, loss_fn, device)\n",
        "        print(f\"[Epoch {epoch+1}] Validation Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "UQNocVnkdlTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for content, style, threshold in dataloader:\n",
        "            content = content.to(device)\n",
        "            style = style.to(device)\n",
        "            threshold = threshold.to(device).view(-1, 1, 1, 1)\n",
        "\n",
        "            output = model(content, threshold)\n",
        "            loss = loss_fn(content, style, output)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "8KLgHuzChk0c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}